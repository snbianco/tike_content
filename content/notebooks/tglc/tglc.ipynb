{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3c25b2-89e3-482e-9472-ed136afb4946",
   "metadata": {},
   "source": [
    "# Accessing TGLC in the Cloud\n",
    "***\n",
    "This notebooks focuses specifically on accessing [TESS-Gaia light curves (TGLC)](https://archive.stsci.edu/hlsp/tglc) in the cloud. However, the approach is applicable to any MAST high level science product.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\n",
    "## Imports\n",
    "We'll need a few packages for this notebook:\n",
    "- `astroquery.mast` to search for and select data\n",
    "- `astropy.io.fits` to read in the fits file\n",
    "- `matplotlib` to create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df010c-ce17-497a-a2d6-317e56175f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.mast import Observations\n",
    "from astropy.io import fits\n",
    "from astropy.timeseries.binned import BinnedTimeSeries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "Observations.enable_cloud_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4a0e3-1c41-47d4-ad92-a3f1c903979c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "As a supplement to data produced by mission teams, MAST also hosts high level science products (HLSPs). HLSPs are community contributed products that complement or enhance our archived data. You can search the metadata for these collections using the [MAST HLSP Search Form](https://mast.stsci.edu/hlsp/#/).\n",
    "\n",
    "TGLC is the first HLSP to be added to our cloud copy of data. In this notebook, we'll examine the light curves for TOI-519, a star with a known exoplanet. We'll compare the photometric precision of TGLC light curves to [standard SPOC pipeline processing](https://ui.adsabs.harvard.edu/abs/2016SPIE.9913E..3EJ/abstract), and discuss why you may prefer to use HLSP data over standard mission data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4a89f-ec2b-422c-aee0-8a85cf876ec8",
   "metadata": {},
   "source": [
    "## Querying for TGLC Data\n",
    "Since this is a query for cloud data, we'll make use of the `get_cloud_uris()` function. This will return the cloud location of matching data products.\n",
    "\n",
    "Our target is TOI-519. We could use `objectname=TOI-519` in the function below, but this is a slower way to search; first, a resolver will need to convert our input to celestial coordinates, then spatially cross-match with a database of millions of observations. Instead, since this object is listed in the TESS Input Catalog as `TIC 218795833`, we can search for targets with that *exact* name. This search will be extremely fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80759998-c598-43d1-a436-3c790f956245",
   "metadata": {},
   "outputs": [],
   "source": [
    "tglc_uri = Observations.get_cloud_uris(target_name=\"218795833\",\n",
    "                                       #objectname=TOI-519,\n",
    "                                       provenance_name='TGLC',\n",
    "                                       sequence_number=[7])\n",
    "tglc_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc94c56-4def-4cdc-b936-527e1dd40c50",
   "metadata": {},
   "source": [
    "We only get one result here, since TGLC packages everything into a single FITS file.\n",
    "\n",
    "## Opening Cloud Data\n",
    "Now that we have the path to the file, we can pass this directly to `fits.open()`; this function will handle reading in data from the cloud quickly and efficiently. It's worth mentioning that:\n",
    "- Since `tglc_uri` is a list, we'll need to add an index `[0]` to get the first element of the list (the filename as a string). `fits.open()` is looking for a string, not a list.\n",
    "- We add `\"anon\":True` to specify anonymous access to cloud data; otherwise, we would need to authorize our access with credentials\n",
    "\n",
    "With our file open, we'll print out the basic FITS structure and load in the lightcurve data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea0236-0cc6-492b-8b7c-c4e6f5329d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(tglc_uri[0], fsspec_kwargs={\"anon\":True}) as f:\n",
    "    f.info()\n",
    "    tglc_data = f[1].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505596c-790e-4108-899a-64343b6b7480",
   "metadata": {},
   "source": [
    "Looking at the output of `f.info()`, it is a little clearer why we called `f[1]`: this is the location of the lightcurve HDU.\n",
    "\n",
    "## Analyzing TGLC Data\n",
    "With our data loaded into `tglc_data`, we can now start to analyze it.\n",
    "\n",
    "### Cleaning and Plotting\n",
    "To begin, we should filter out any datapoints which are \"bad\". This is indicated by data quality flags, where a value of zero indicates no flags have been set. Excluding all data which has any flag set could potentially prune some good data, but it's good enough for a first pass.\n",
    "\n",
    "Since TGLC includes its own data quality flags, alongside those from the mission pipeline, we should include both in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd6f43-f71c-4353-8f59-d3b348e85dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TGLC includes its own quality flags AND those from the mission pipeline\n",
    "tess_flag = tglc_data['TESS_flags'] == 0\n",
    "tglc_flag = tglc_data['TGLC_flags'] == 0\n",
    "\n",
    "# keep only the data which is \"good\" for both\n",
    "good_flags = np.bitwise_and(tess_flag, tglc_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa6511f-fe4d-49f9-8c3e-24a9678347a2",
   "metadata": {},
   "source": [
    "With this \"good\" data marker available, we can now extract our data and create a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6561ae-5bd9-4520-81ef-8578d4fdc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the time and psf flux data\n",
    "tglc_time = tglc_data['time'][good_flags]\n",
    "tglc_flux = tglc_data['psf_flux'][good_flags]\n",
    "\n",
    "# set up the plot, add some labels\n",
    "plt.plot(tglc_time, tglc_flux)\n",
    "plt.title('TOI 519 b - TESS Sector 7')\n",
    "plt.xlabel('TBJD')\n",
    "plt.ylabel('Flux e-/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d7644-e158-4edb-8c5a-418e044a3c51",
   "metadata": {},
   "source": [
    "The exoplanet transit is quite clear in this data!\n",
    "\n",
    "### Calculating Photometric Precision\n",
    "We'll loosely borrow the strategy for calculating precision from the original [TGLC Paper](https://ui.adsabs.harvard.edu/abs/2023AJ....165...71H/abstract), specifically the strategy described on page 10. Equation 5 relates the precision to `D`, the flux difference between adjacent fluxes:\n",
    "$$\\text{precision}=\\frac{1.48}{\\sqrt 2} \\text{median(|D|)}$$\n",
    "\n",
    "We can write a function for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0c2b8-92b5-4975-89f8-41777062f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the flux as an array\n",
    "def precision(lc_flux):\n",
    "    # subtracting offset arrays gives us the adjacent flux differences\n",
    "    flux_diff = lc_flux[1:]-lc_flux[:-1]\n",
    "    return 1.48/np.sqrt(2)*np.nanmedian(np.abs(flux_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8dc578-cdfb-4795-b0a9-c8e18c17a9bd",
   "metadata": {},
   "source": [
    "Let's use this function to calculate the photometric precision of our flux. By converting first to normalized flux, we can write the precision as a percent; this will be useful to us later on.\n",
    "\n",
    "**Note**: the paper calls for a weighted average of the PSF and aperture flux. For simplicity, we are not doing this, but it will impact our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e159e92-12ee-4f38-b4e7-355ffe89d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the flux\n",
    "tglc_flux_norm = tglc_flux/np.nanmean(tglc_flux)\n",
    "\n",
    "# calculate precision, convert to percent\n",
    "precision(tglc_flux_norm)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce374f-e421-4dcc-b8bf-773e23e0d47b",
   "metadata": {},
   "source": [
    "Our precision is thus 0.729%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d13a34-54d3-4273-a423-d9351b9d0cd5",
   "metadata": {},
   "source": [
    "## Comparing to SPOC Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f7030-9b31-4867-a876-3042068344e0",
   "metadata": {},
   "source": [
    "The [Science Processing Operations Center (SPOC)](https://ui.adsabs.harvard.edu/abs/2016SPIE.9913E..3EJ/abstract) produces the official TESS mission light curves. Although these light curves are great for their intended purpose, they don't typically reach the levels of photometric precision of TGLC.\n",
    "\n",
    "Let's repeat our steps from before, but this time use the SPOC data. One caveat here is that SPOC data are separated over many files: there are light curves, target pixel files, validation reports, and more. In order to get a single result (the lightcurve file) from `get_cloud_uris`, we'll need to pass a value to `filter_products`. For this example, `\"description\":\"Light curves\"` will return our desired result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d9703-2165-4da7-8e07-4fe667e7fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify our query for SPOC; add new filter for light curves\n",
    "spoc_uri = Observations.get_cloud_uris(target_name=\"218795833\",\n",
    "                                       provenance_name='SPOC',\n",
    "                                       sequence_number=[7],\n",
    "                                       filter_products = {\"description\":\"Light curves\"}\n",
    "                                      )\n",
    "# print info, load in the data\n",
    "with fits.open(spoc_uri[0], fsspec_kwargs={\"anon\":True}) as f:\n",
    "    f.info()\n",
    "    spoc_data = f[1].data\n",
    "    spoc_head = f[1].header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad92ca9-45fe-4200-8930-4535353c5df5",
   "metadata": {},
   "source": [
    "Notice that this file is structured differently from the TGLC file; it has a secondary aperture HDU. This contains information on which pixels were used for simple aperture photometry.\n",
    "\n",
    "Another important caveat is the cadence of observations: TGLC is based on the FFIs, while SPOC uses higher-cadence data. For sector 7, this is the difference between a data point every 30 minutes and one every 2 minutes. We can verify the SPOC cadence using the `\"TIMEDEL\"` keyword from the header. We'll do a little conversion to see the value in minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c2bfb-63c5-4a77-bce6-c2fca872519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time delta, in days, times 24 hours per day, times 60 min per hour\n",
    "spoc_head['TIMEDEL']*24*60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088172cd-5fbb-44fa-9094-e5d794db6479",
   "metadata": {},
   "source": [
    "So how do we adjust the time resolution? Well, there are [tools in astropy for binning timeseries data](https://docs.astropy.org/en/stable/api/astropy.timeseries.BinnedTimeSeries.html), but they're a bit opaque for this tutorial. What we'd really like to do is take every 15 data points and compute their average value. To accomplish that, we'll:\n",
    "\n",
    "1. Get every 15th value from the time series.\n",
    "2. \"Zip\" the values up together so that consecutive values are in the same array. That is, we will have an Nx15 matrix, where each row contains 15 values that were adjacent in our original timeseries. If our initial timeseries length is not a multiple of 15, we'll lose some values at the end; that's ok, since this is a tutorial and not a scientific publication.\n",
    "3. Take the mean of the zipped values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806560e-b92d-4f36-9ac4-15b88b5f3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, get the flux and time data\n",
    "spoc_time = spoc_data[\"TIME\"]\n",
    "spoc_flux = spoc_data[\"PDCSAP_FLUX\"]\n",
    "\n",
    "# get every 15th value from each timeseries\n",
    "mega_tarray = [spoc_time[i::15] for i in range(15)]\n",
    "mega_farray = [spoc_flux[i::15] for i in range(15)]\n",
    "\n",
    "# this looks complicated, but is just steps 2 and 3 above:\n",
    "# zip the values, then sum each row of the matrix\n",
    "newt = np.array(list(zip(*mega_tarray))).mean(axis=1)\n",
    "newf = np.array(list(zip(*mega_farray))).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71dd1f-255d-43ea-b522-d7b8fe09f1d3",
   "metadata": {},
   "source": [
    "If we normalize this data, we can calculate the photometric precision and compare with our earlier result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66a0a1-71fd-4e2a-aa11-f11006e62f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spoc_time = newt\n",
    "spoc_flux_norm = newf/np.nanmean(newf)\n",
    "\n",
    "# calculate precision\n",
    "precision(spoc_flux_norm)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718d698-0506-4c61-a4ef-ad3b05668edc",
   "metadata": {},
   "source": [
    "The precision for SPOC processing is 0.795%, which is slightly worse than the 0.729% from TGLC. This is reflective of TGLC's better performance with faint stars in crowded fields!\n",
    "\n",
    "The reduced scatter is somewhat visible if we plot the data together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8acbe6-eece-4225-8dae-1d25ef9a80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(spoc_time, spoc_flux_norm)\n",
    "plt.plot(tglc_time, tglc_flux_norm, c='orange')\n",
    "\n",
    "plt.legend([\"SPOC\", \"TGLC\"])\n",
    "plt.title('TOI 519 b - TESS Sector 7')\n",
    "plt.xlabel('TBJD')\n",
    "plt.ylabel('Normalized flux')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331b897-143b-42e9-8b6e-8d1dca969e04",
   "metadata": {},
   "source": [
    "TGLC, in orange, does slightly better. You might find this reduced noise useful when looking at particularly dim stars!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af0fdb-0266-4403-bd0d-5ea44dad649e",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- [MAST HLSP Search Form](https://mast.stsci.edu/hlsp/#/)\n",
    "- [TGLC Paper](https://ui.adsabs.harvard.edu/abs/2023AJ....165...71H/abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d1ea9-d1ed-492f-bf33-3df75431c020",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "\n",
    "If you have comments or questions on this notebook, please contact us through the Archive Help Desk e-mail at archive@stsci.edu. If you spot any errors, open an issue on the [tike_content repository](https://github.com/spacetelescope/tike_content).\n",
    "\n",
    "**Authors:** Thomas Dutkiewicz, Te Han <br>\n",
    "**Keywords:** TIKE, AWS, TESS <br>\n",
    "**Last Updated:** Mar 2025 <br>\n",
    "\n",
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
